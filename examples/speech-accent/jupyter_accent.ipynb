{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from zeno import zeno\n",
    "import math\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeno({\n",
    "    \"metadata\": df[0:10],\n",
    "    \"view\": \"audio-transcription\",\n",
    "    \"data_path\": \"/Users/acabrera/dev/data/speech-accent-archive/recordings/recordings/\",\n",
    "    \"label_column\": \"label\",\n",
    "    \"data_column\": \"id\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from jiwer import wer\n",
    "from zeno import ZenoOptions, distill, metric, model\n",
    "import numpy as np\n",
    "from zeno import ZenoOptions, distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model\n",
    "def load_model(model_path):\n",
    "    if \"sst\" in model_path:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model, decoder, utils = torch.hub.load(\n",
    "            repo_or_dir=\"snakers4/silero-models\",\n",
    "            model=\"silero_stt\",\n",
    "            language=\"en\",\n",
    "            device=device,\n",
    "        )\n",
    "        (read_batch, _, _, prepare_model_input) = utils\n",
    "\n",
    "        def pred(df, ops: ZenoOptions):\n",
    "            files = [os.path.join(ops.data_path, f) for f in df[ops.data_column]]\n",
    "            input = prepare_model_input(read_batch(files), device=device)\n",
    "            return [decoder(x.cpu()) for x in model(input)]\n",
    "\n",
    "        return pred\n",
    "\n",
    "    elif \"whisper\" in model_path:\n",
    "        model = whisper.load_model(\"tiny\")\n",
    "\n",
    "        def pred(df, ops: ZenoOptions):\n",
    "            files = [os.path.join(ops.data_path, f) for f in df[ops.data_column]]\n",
    "            outs = []\n",
    "            for f in files:\n",
    "                outs.append(model.transcribe(f)[\"text\"])\n",
    "            return outs\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "@distill\n",
    "def country(df, ops: ZenoOptions):\n",
    "    if df[\"0birthplace\"][0] == df[\"0birthplace\"][0]:\n",
    "        return df[\"0birthplace\"].str.split(\", \")[-1][-1]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "@distill\n",
    "def wer_m(df, ops: ZenoOptions):\n",
    "    return df.apply(lambda x: wer(x[ops.label_column], x[ops.output_column]), axis=1)\n",
    "\n",
    "\n",
    "@metric\n",
    "def avg_wer(df, ops: ZenoOptions):\n",
    "    avg = df[ops.distill_columns[\"wer_m\"]].mean()\n",
    "    if math.isnan(avg):\n",
    "        return 0\n",
    "    return avg\n",
    "\n",
    "# @distill\n",
    "# def amplitude(df, ops: ZenoOptions):\n",
    "#     files = [os.path.join(ops.data_path, f) for f in df[ops.data_column]]\n",
    "#     amps = []\n",
    "#     for audio in files:\n",
    "#         y, _ = librosa.load(audio)\n",
    "#         amps.append(float(np.abs(y).mean()))\n",
    "#     return amps\n",
    "\n",
    "\n",
    "# @distill\n",
    "# def length(df, ops: ZenoOptions):\n",
    "#     files = [os.path.join(ops.data_path, f) for f in df[ops.data_column]]\n",
    "#     amps = []\n",
    "#     for audio in files:\n",
    "#         y, _ = librosa.load(audio)\n",
    "#         amps.append(len(y))\n",
    "#     return amps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zeno({\n",
    "    \"metadata\": df,\n",
    "    \"functions\": [load_model, country, wer_m, avg_wer],\n",
    "    \"view\": \"audio-transcription\",\n",
    "    \"models\": [\"silero_sst\", \"whisper\"],\n",
    "    \"data_path\": \"/Users/acabrera/dev/data/speech-accent-archive/recordings/recordings/\",\n",
    "    \"data_column\": \"id\",\n",
    "    \"label_column\": \"label\",\n",
    "    \"samples\": 10,\n",
    "})\n",
    "# metadata = \"metadata.csv\"\n",
    "# # data_path = \"https://zenoml.s3.amazonaws.com/accents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "59d606a796fde3c997548ee5ab3f3009081de8aa2fb58c2406e58b3c7613e786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
